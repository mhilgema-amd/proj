#!/bin/bash
#SBATCH -J rochpl.g42.hermes-2.8n.8ppn.8gpu.pre-test
#SBATCH -o %x.%N.o%j
#SBATCH -N 8
#SBATCH -n 64
#SBATCH -c 16
#SBATCH --gres=gpu:8
#SBATCH -p hermes-2
#SBATCH -t 2:00:00
#SBATCH --mail-user=martin.hilgeman@amd.com
#SBATCH --mail-type=begin  # email me when the job starts
#SBATCH --mail-type=end    # email me when the job finishes
#SBATCH --exclusive
##SBATCH --exclude=auh7-3b-gpu-224,auh7-3b-gpu-186,auh7-3b-gpu-192

#
# Excluded nodes: auh7-3b-gpu-033 - Needs reboot (amdgpu driver)
#                 auh7-3b-gpu-224 - check FAILED for minihpcg
#                                   Device 6:: FAILED due to performance below requirement (1614.61, GB/s)
#                 auh7-3b-gpu-186 - Cannot SSH into the node
#                 auh7-3b-gpu-192 - No ROCm capable device detected (4-7)
#

alias mytime='/usr/bin/time -f "Elapsed: %e  User: %U  System: %S  PageF: %F"'
shopt -s expand_aliases
set -o pipefail

#
# Definition of the environment
#
function init_env()
{
    #
    #
    # Edit PREFIX to the directory where you unpacked the archive
    #
    APP=rochpl
    COMPILER=gcc114
    MPI=openmpi
    FRAMEWORK=rocm633
    VERSION=default

    #
    # Directory with custom Slurm prologue/epilogue/profiling scripts
    #
    # SCRIPT_DIR is used by other scripts, needs to be exported
    #
    export SCRIPT_DIR="${HOME}/martinh/app/slurm/bin"

    #
    # Number of MPI ranks
    # set it to 1 per CCX for a single node run
    #
    NPES=${SLURM_NTASKS}
    NNODES=${SLURM_NNODES}
    PPN=$((NPES / NNODES))
    NGPUS=${SLURM_GPUS_ON_NODE:-${PPN}}
    OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}

    #
    # Standard paths
    #
    # PREFIX and SCR are used by other scripts, need to be exported
    #
    export PREFIX=${HOME}/martinh/app/${APP}
    INPD=${PREFIX}/inp
    INP="${INPD}/HPL.dat"
    export SCR="${HOME}/martinh/scratch/${APP}/${SLURM_JOB_ID}"
}

#
# Load modules
#
function load_modules ()
{
    if which module; then
        if [ -d ${HOME}/opt/modulefiles ]; then
            module use ${HOME}/opt/modulefiles
        fi
        MOD=${APP}/${COMPILER}/${MPI}/${FRAMEWORK}/${VERSION}
        if module is-avail ${MOD}; then
            module load ${MOD}
        else
            echo "Unable to find module ${MOD}. Exiting"
            exit 1
        fi
        module list
    else
        . ${HOME}/martinh/env_mpi.sh || exit 1
    fi

    echo "#### LD_LIBRARY_PATH ####"
    echo ${LD_LIBRARY_PATH}
    echo
}

#
# Initialize the job environment and setup the nodes
#
function init_job ()
{
    #
    #
    # List of nodes
    echo "#### Node list ####"
    scontrol show hostnames
    echo
}

#
# Set scratch directory
#
function setup_scratch ()
{
    if [ ! -d "${SCR}" ]; then
        mkdir -p "${SCR}" || exit 1
    fi
    cd "${SCR}" || exit 1

    #
    # Copy input files
    #
    #cp ${INP} HPL.dat || exit 1 # Using command line arguments
}

#
# Setup the environment (Application/MPI/UCX/...)
#
function setup_env ()
{
    #
    # MPI
    #
    MPIRUN=mpirun
    MPI_ARGS="${MPI_ARGS} -np ${NPES}"
    MPI_ARGS="${MPI_ARGS} --bind-to none"
    MPI_ARGS="${MPI_ARGS} --mca coll_hcoll_enable 0"
    MPI_ARGS="${MPI_ARGS} --mca psec ^munge"
    MPI_ARGS="${MPI_ARGS} --mca pml ucx"

    #
    # UCX stuff
    #
    MPI_ARGS="${MPI_ARGS} -x UCX_TLS=self,sm,rocm_copy,rocm_ipc,ud_verbs"
    MPI_ARGS="${MPI_ARGS} -x UCX_RNDV_THRESH=32768"
    MPI_ARGS="${MPI_ARGS} -x UCX_IB_GID_INDEX=3"
    MPI_ARGS="${MPI_ARGS} -x UCX_PROTO_INFO=n"
    MPI_ARGS="${MPI_ARGS} -x UCX_PROTO_ENABLE=y"
    MPI_ARGS="${MPI_ARGS} -x UCX_UNIFIED_MODE=y"
    MPI_ARGS="${MPI_ARGS} -x UCX_ROCM_COPY_LAT=2e-6"
    MPI_ARGS="${MPI_ARGS} -x UCX_ROCM_IPC_MIN_ZCOPY=4096"
    MPI_ARGS="${MPI_ARGS} -x UCX_RNDV_SCHEME=get_zcopy"

    #
    # AMD GPU setttings
    #
    #export ROCR_VISIBLE_DEVICES=0,1

    #
    # Affinity
    #
    AFFINITY=${HOME}/martinh/bin/affinity.sh
    unset SLURM_CPU_BIND

    #
    # Application specific stuff
    #
    NB=512
    P=8
    Q=8
    N=724992 # 95.5% of memory
    EXE=rochpl
    EXE_ARGS="-P ${P} -Q ${Q} -N ${N} --NB ${NB} -p 2 -q 4"

    #
    # NCCL/RCCL
    #
    export NCCL_DEBUG=INFO
    #export NCCL_DEBUG_SUBSYS="INIT,ENV,GRAPH,NET,TUNING"
    #export NCCL_TOPO_DUMP_FILE="${SCR}/rccl-${SLURM_JOBID}.xml"
    export NCCL_NET_GDR_LEVEL=3
    export RCCL_MSCCL_FORCE_ENABLE=0
    export RCCL_MSCCLPP_ENABLE=0
    export NCCL_CROSS_NIC=1
}

#
# Write node configuration to disk
#
function dump_system_info ()
{
    SYSINFO="${SCRIPT_DIR}/my_sysinfo.sh"
    if [ ! -f "${SYSINFO}" ]; then
        echo "SYSINFO script ${SYSINFO} does not exist. Exiting."
        exit 1
    fi

    echo "RUNNING SYSINFO script ${SYSINFO} in the background ..."
    srun -N ${NNODES} -n ${NNODES} ${SYSINFO}
}

#
# Job prologue to setup nodes properly
#
function run_prologue ()
{
    PROLOGUE="${SCRIPT_DIR}/prologue.sh"
    if [ ! -f "${PROLOGUE}" ]; then
        echo "Prologue script ${PROLOGUE} does not exist. Exiting."
        exit 1
    fi

    #
    # Settings for prologue script
    #
    export BOOST_EN=1
    export CPU_CLK_MAX=3700MHz
    export SCLK_MAX=2100
    export SCLK_MIN=2000

    echo "RUNNING PROLOGUE script ${PROLOGUE}"
    srun -N ${NNODES} -n ${NNODES} ${PROLOGUE} || scancel ${SLURM_JOB_ID}
} 

#
# Run the application
#
function run_app ()
{
    #
    # Store environment
    #
    env > ${SCR}/env.txt 2>&1

    #
    # ldd of the executable
    #
    echo
    echo "ldd $( which ${EXE} )"
    ldd $( which ${EXE} )

    RUNCMD="${MPIRUN} ${MPI_ARGS} ${AFFINITY} ${EXE} ${EXE_ARGS}"
    echo
    echo "${RUNCMD}"
    mytime ${RUNCMD} 2>&1 | tee stdout.${APP}.${SLURM_JOB_ID}
}

#
# Start the PM log
#
function start_agt ()
{
    AGT="${SCRIPT_DIR}/my_agt.sh"
    export AGT_JOBNAME="my_agt_job.${SLURM_JOB_ID}"
    touch ${SCR}/${AGT_JOBNAME}

    if [ ! -f "${AGT}" ]; then
        echo "AGT script ${AGT} does not exist. Exiting."
        exit 1
    fi

    echo -ne "RUNNING AGT script ${AGT} with job name ${AGT_JOBNAME} in the background ...\n"
    srun --job-name=${AGT_JOBNAME} -N ${NNODES} -n ${NNODES} ${AGT} &
}

#
# Stop the PM log
#
function stop_agt ()
{
    if [ ! -f "${AGT}" ]; then
        echo "AGT script ${AGT} does not exist. Exiting."
        exit 1
    fi

    echo "STOPPING AGT script ${AGT} ..."
    rm ${SCR}/${AGT_JOBNAME}
}

#
# Run checks pior to the big run
#
function do_checks ()
{
    DO_CHECKS="${SCRIPT_DIR}/do_checks.sh"

    if [ ! -f "${DO_CHECKS}" ]; then
        echo "Node check script ${DO_CHECKS} does not exist. Exiting."
        exit 1
    fi

    echo -ne "RUNNING DO_CHECKS script ${DO_CHECKS} ...\n"
    srun -N ${NNODES} -n ${NNODES} ${DO_CHECKS} || scancel ${SLURM_JOB_ID}
}

#
# Job epilogue to setup nodes properly
#
function run_epilogue ()
{
    EPILOGUE="${SCRIPT_DIR}/epilogue.sh"
    if [ ! -f "${EPILOGUE}" ]; then
        echo "Epilogue script ${EPILOGUE} does not exist. Exiting."
        exit 1
    fi

    echo "RUNNING EPILOGUE script ${EPILOGUE}"
    srun -N ${NNODES} -n ${NNODES} ${EPILOGUE} || scancel ${SLURM_JOB_ID}
} 

init_env
load_modules
init_job
setup_scratch
setup_env
dump_system_info
#run_prologue
#start_agt
do_checks
run_app
#stop_agt
#run_epilogue
